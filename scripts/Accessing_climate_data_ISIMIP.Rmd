---
title: "Accessing data from ISIMIP repository"
author: "Denisse Fierro Arcos"
date: "2023-08-19"
output: 
  github_document:
    toc: true
    html_preview: false
---

# Introduction
This notebook will accessing data from the [Inter-Sectoral Impact Model Intercomparison Project (ISIMIP) Repository](https://data.isimip.org/). There are various datasets available, but as example we will be accessing Input Data > Climate Forcing > ISIMIP3a simulation round. We will use the sea surface temperature (`tos`) monthly global output from the GFDL-MOM6-COBALT2 model.  

In this notebook, we will use `Python` to make use of some functions from the `isimip-client` library, which you should have already installed in your local machine if you followed the instructions in the `README` file. If you have not done this yet, make sure you follow these instructions before running this notebook.  
  
After we have downloaded the data we need with the `isimip-client` library, we will then move to `R` to visualise the data.

## Loading R libraries
All libraries used in this notebook should already be installed in your local machine. Make sure you follow the instructions in the `README` file before running this notebook.  

```{r results = "hide", warnings = F, message = F}
library(reticulate)
library(tidyverse)
library(metR)
library(lubridate)
library(raster)
library(sf)
```

## Using Python in an R notebook
We will use the `reticulate` package to call `Python` in this notebook. Before you run this, make sure you have edited the `.Rprofile` file following the instructions in the README.  

```{r warnings = F, message = F}
#Calling a specific conda environment
use_condaenv("fishmip")
```

## Loading ISIMIP Client script
We can call the `isimip-client` library and load it into `R` as shown below. We can then use the `$` sign to call the different modules available in the library.  
  
```{r}
#Loading isimip-client into R
cl <- import("isimip_client.client")
```

```{python}
#Loading the library
import isimip_client.client as cl

#Starting a session
client = cl.ISIMIPClient()
```

  
## Starting an `isimip-client` session
By starting a session we can query the ISIMIP database. We will look for climate data (considered as Input Data) from the ISIMIP3a simulation. We will search for monthly sea surface temperature (`tos`) outputs from the GFDL-MOM6-COBALT2 earth system model.  
  
There are several parameters available to perform a search. It is beyond the scope of this training session to cover all options, but the best way to familiarise yourself with the parameters available is by searching a variable of your interest in the ISIMIP Repository website. You can click [here](https://data.isimip.org/datasets/d7aca05a-27de-440e-a5b2-2c21ba831bcd/) for the results of the search described here. The parameters used here can be seen under the `Specifiers` section in the link above.  
  
```{python}
#Starting a query - Looking for climate inputs from the ISIMIP3a simulation
query = client.datasets(simulation_round = 'ISIMIP3a',\
                           product = 'InputData',\
                           category = 'climate',
                           climate_forcing = 'gfdl-mom6-cobalt2',\
                           climate_scenario = 'obsclim',\
                           subcategory = 'ocean',\
                           region = 'global',\
                           time_step = 'monthly',\
                           resolution = '60arcmin',\
                           climate_variable = 'tos')

query
```
  
We can check the number of results we obtained from our query.  
  
```{python}
query['count']
```
  
Our query produced two results. These are stored as a list. We can check the information included in our query by typing `query$results`. But for now, we will check the names of the variables included in our search.  
  
```{python}
query['results'][0]
```
  
We can check some metadata for all the results with the following code.
  
```{python}
for ds in query['results']:
  print(ds['name'], ds['files'])

```
  
We can see that the results include two horizontal resolutions: 15 arcmin ($0.25^{\circ}$) and 60 arcmin ($1^{\circ}$).  
  
It is worth noting that the files in the search results include data for the entire planet as the earth system models are global in extent. If you are interested in extracting data for a specific region, you can subset the global data to the area of your interest. Before extracting the regional data, we will need the URL for the location of the datasets. 
  
For this example, we will use the boundaries of the Hawaiian Longline region, which we have provided in the `data` folder.
  
```{python}
#Empty lists to save URLs linking to files
urls = []
urls_sub = []

#Looping through each entry available in search results
for datasets in query['results']:
  for paths in datasets['files']:
    urls.append(paths['file_url'])
    urls_sub.append(paths['path'])

```
  
### Extracting data for a region
First, we will load the Hawaiian Longline region and extract the bounding box. We will use the `sf` library to do this. Then, we will use this region to subset the data we need with the `isimip-client` library.
  
```{r}
#Loading region of interest
region_shp <- read_sf("../data/HawaiianLongline_BordersClip.shp")
#Getting bounding box (max and min coordinates)
region_bbox <- st_bbox(region_shp)
bbox <- c(region_bbox$ymin, region_bbox$ymax, region_bbox$xmin, region_bbox$xmax)
bbox
```
  

If you are looking for a subset of the global dataset, you can set a bounding box and only extract data for your area of interest. In this case, we will extract information for the Southern Ocean only.

```{python}
#We use the cutout function to create a bounding box for our dataset
HI_data_URL = client.cutout(urls_sub, bbox = r.bbox)
HI_data_URL
```

## Downloading data to disk
We will download the data and store it into the `data` folder. First we will make sure a `data` folder exists and if it does not exist, we will create one.

```{python}
#Importing library to check if folder exists
import os

#Creating a data folder if one does not already exist
if os.path.exists('../data/') == False:
  os.makedirs('../data/')
else:
  print('Folder already exists')

```
Use the `client.download()` function to save data to disk. 
  
```{python eval = F}
#To download the subsetted data
client.download(url = HI_data_URL['file_url'], \
                path = '../data/', validate = False, \
                extract = True)
```
  
To download global data we use the same function, but we need to point at the correct variable storing the URL to the global dataset.  
  
```{python eval = F}
client.download(url = urls[0], \
                path = '../data/', validate = False, \
                extract = True)
                
```
  
# R-based code
You are now ready to load the dataset into `R` to make any calculations and visualise results.  
  
## Inspecting contents of netcdf file
For a quick overview of the content of the dataset we just downloaded, we can make use of the `metR` package.  

```{r}
#Provide file path to netcdf that was recently downloaded.
data_file <- list.files(path = "../data/", pattern = "nc$", full.names = T)

#Check contents of netcdf
GlanceNetCDF(data_file)
```
  
This output, however, does not give you information about `No Data Values`. So we will load the first timestep included in our dataset to obtain this information.  
  
We will also plot the data to inspect it quickly.  
  
```{r}
#Loading the first timestep as raster
sst_raster <- brick(data_file, band = 1)

#Extracting missing values
NA_val <- sst_raster@file@nodatavalue

#Plotting first time step of raster
plot(sst_raster[[1]])
```
We can see that a `No Data Values` is included in the dataset to mark areas where no values have been collected because all land areas have the same value.  
  
We can create a mask for the `No Data Values` and plot the raster again.

```{r}
#Changing values larger than No Data Value to NA
sst_raster[sst_raster >= NA_val] <- NA

#Plotting result
plot(sst_raster[[1]])
```
  
## Masking temperature using region shapefile
This will allow us to keep data grid cells inside the boundaries of our region of interest.  
  
```{r}
raster_crop <- mask(sst_raster, region_shp)

#Plotting result
plot(raster_crop[[1]])
```
  
## Loading dataset as dataframe for easy manipulation
Data frame allow us to perform calculations easily using the `tidyverse`. We will also need to decode the time steps for the raster. If you look at the results of `GlanceNetCDF(data_file)` above, you will see that time was given as `months since 1901-01-01 00:00:00`. In this step, we transform the number of months to a date before calculating a time series for this region.  
  
```{r}
#Turning raster into a matrix
sst_hi <- rasterToPoints(raster_crop) %>% 
  #Changing to data frame
  as.data.frame() %>% 
  #Reshaping data frame
  pivot_longer(cols = -c(x, y), names_to = "months_from_date") %>% 
  #Removing the X before the number of months and turning into numeric data
  mutate(months_from_date = as.numeric(str_remove(months_from_date, "X")),
         #Calculate date
         date = ymd("1901-01-01") %m+% months(months_from_date)) %>% 
  #Removing the months column
  dplyr::select(!months_from_date)

#Checking results
head(sst_hi)
```
  
## Calculating climatology
We will use all data between `r year(min(sst_hi$date))` and `r `year(max(sst_hi$date))` to calculate the monthly SST mean for the Hawaiian Longline region.  
  
```{r}
clim_sst <- sst_hi %>% 
  #Calculating climatological mean for total catch per pixel
  group_by(date) %>% 
  summarise(mean_sst = mean(value, na.rm = F))

#Checking results
head(clim_sst)
```

## Plotting climatology

```{r}
#Plotting data
clim_sst %>% 
  #Show date in x axis and SST in y axis
  ggplot(aes(x = date, y = mean_sst))+
  #We will use a line plot
  geom_line(color = 'blue')+
  #and a dot plot
  geom_point()+
  #We will change the dates to show as month and year every 2 years
  scale_x_date(date_labels="%b-%Y", date_breaks = "2 years")+
  #We will change the axes labels and title
  labs(x = NULL, y = expression("Mean monthly SST  " (degree~C)),
       main = "Mean monthly sea surface temperature for the Hawaiian Longline region")+
  #We will apply a predetermined theme that removes background
  theme_bw()+
  #Showing axis label at an angle
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```